---
title: "Reproducible Research: Peer Assessment 1"
author: John ARNZEN
output: 
  html_document:
    keep_md: yes
---

Please see [README.md](README.md) for a description of this assignment. Also please note that not all of its content is the same as the more up-to-date assignment description on coursera.org---I've used the coursera content for step headings and labels.

**One more note**: the default directory for figures in my installation of R/RStudio + knitr is `PA1_template_files\figure-html\`, not `figures\` as mentioned in the assignment. I did not change this location.

## Loading and preprocessing the data

### 1. Load the data (i.e. read.csv())

**We assume** that `activity.zip` has been downloaded and its content (`activity.csv`) has been unzipped into the same directory as this R markdown file.

We load the data into a data table. Take a first look at the head and tail of it, and the column data types inferred by `fread()`.

```{r}
library(data.table, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)

(dt <- fread("activity.csv"))
str(dt)
```

### 2. Process/transform the data (if necessary) into a format suitable for your analysis

We see that the `date` column is currently just a string, although it is in ISO date format. We convert that column to R's `Date` type, and then get a summary view of the data table's columns. We're particularly interested in the statistics for the `steps` column.

```{r}
dt$date <- as.Date(dt$date)

summary(dt)
```

NOTE the `NA's` count for `steps`. We'll verify that count below.

## What is mean total number of steps taken per day?

### 1. Calculate the total number of steps taken per day

We start addressing this question by creating and processing a new data table based on our initial data table.

We first strip all incomplete cases from original dataset---we do this so that we're dealing only with non-NA steps values in our statistics. Keeping the NA steps can affect the data in one of two ways:

* if the statistics parameter `na.rm = FALSE`, then the statistics will result in `NA` if it encounters any NAs, invalidating the statistic
* if the statistics parameter `na.rm = TRUE`, then dates with only NAs will return `0` (zero), which may not be correct and will increase the zero-bin count in the histogram

Either way our statistics will be skewed. While in our original data table it looks like all NAs occur in all intervals of certain days---meaning the day itself is effectively NA---there may be some chance that there's a `steps` value hidden amongst NAs for a day, so let's increase our chances of working with all valid data.

We then group the complete cases by `date` and then summarise the data into a new table containing dates and summations of steps per date. Just to spot-check the values in the new table, we get it's summary afterwards.

```{r}
(ts <- na.omit(dt) %>% 
     group_by(date) %>% 
     summarise(total_steps = sum(steps, na.rm = TRUE)))
summary(ts)
```

### 2. Make a histogram of the total number of steps taken each day

We use `ggplot2` to create the histogram (and other plots), assigning a `binwidth` to a range of 500 tallies to give us a decent granularity in the histogram.

```{r}
library(ggplot2, warn.conflicts = FALSE)

(p_ts <- ggplot(ts) + 
    geom_histogram(aes(total_steps), binwidth = 500) +
    labs(title = "Histogram of Total Steps per Day",
         x = "Total steps per day"))
```

### 3. Calculate and report the mean and median of the total number of steps taken per day

This is fairly straight forward to do. We'll save these values in variables so that we can reuse them later if needed.

```{r}
(ts_mean <- mean(ts$total_steps, na.rm = TRUE))
(ts_median <- median(ts$total_steps, na.rm = TRUE))
```

Let's re-visualize the histogram from above with the median (solid red line) and mean (dashed red line) included.

```{r}
(p_ts_stats <- p_ts + 
    geom_vline(aes(xintercept = ts_median), color = "red", linetype = "solid") +
    geom_vline(aes(xintercept = ts_mean), color = "red", linetype = "dashed"))
```

In this case, the mean and median are so close that they essentially overlap in our plot.

## What is the average daily activity pattern?

### 1. Make a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all days (y-axis)

We start by creating a data table to answer this question, following a path similar to what we did for the histogram above.

We strip out all of the NAs from the original table, group the results by the `interval`s, and then create a new summarized table containing the average of each interval across all days. (Just for comparison, we also calculate the median of the steps per interval.)

```{r}
(it <- na.omit(dt) %>% 
     group_by(interval) %>% 
     summarise(avg_steps = mean(steps, na.rm = TRUE),
               median_steps = median(steps, na.rm = TRUE)))
```

Let's get a look ranges and quantiles.

```{r}
summary(it)
```

Now let's plot it out. Because the `interval`s are numeric representations of time of day (hours and minutes), `ggplot()` will think the intervals are continuous instead of discrete. This will introduce gaps in the graph between the 55 and the next hundred value. To get around this we coerce the intervals into a factor and then adjust the X-axis scale to show ticks for every 2 hours. To help with the visualization, we also include points on the line.

```{r}
ggplot(it, aes(as.factor(interval), avg_steps)) + 
    geom_line(aes(group = 1)) +
    geom_point() +
    scale_x_discrete(breaks = seq(0, 2400, 200)) +
    labs(title = "Average Steps per Interval, Across All Days",
         x = "Intervals (time of day, 5 minute intervals)",
         y = "Average steps")
```

### 2. Which 5-minute interval, on average across all the days in the dataset, contains the maximum number of steps?

We get the interval of interest by filtering the data table to where the maximum average occurs.

```{r}
it[avg_steps == max(avg_steps)]
```

## Imputing missing values

### 1. Calculate and report the total number of missing values in the dataset (i.e. the total number of rows with NAs)

There are myriad ways of doing this. We'll use the facilities of our data table to tell us the count, using `data.table`'s special variable `.N` to give us the count of rows based on the row filter in the first indexing position.

```{r}
dt[is.na(steps), .N]
```

NOTE that this matches the NA count in the `summary(dt)` output above.

### 2. Devise a strategy for filling in all of the missing values in the dataset. The strategy does not need to be sophisticated. For example, you could use the mean/median for that day, or the mean for that 5-minute interval, etc.

We have a number of ways to accomplish this, and our strategy should look at where and how the NAs distribute within the data.

A quick manual perusal of the NAs, and calculations on their count compared to the overall data table, seems to indicate that the NAs occur in all intervals for certain days, effectively making the days themselves NA. Thus we have some consistency in the NA groupings.

We're still working with intervals per day though, so it wouldn't make much sense to use the daily average for each interval without severely skewing the results.

* We could divide the daily average by the number of intervals per day to distribute the average uniformly across the day. We could do the same with the daily median.
* We could fill the NAs with zero values, but then we'd be making assumptions that missing values mean no movement.
* We could use some form of mean or median for each interval to fill the NAs.
    * Using the interval means might work, but it might also affect the variance and standard deviation of the data adversely.
    * Using the interval medians might work better and be safer.
* We could also use a imputation library like `zoo` or `mice` and their functions, but that might defeat the purpose of doing this manually for this exercise.

For this assignment, let's use the **median steps per interval** that we calculated with the means above to fill in our NAs and see what happens.

### 3. Create a new dataset that is equal to the original dataset but with the missing data filled in.

We create a copy of the original data table because `data.table` seems to assign by reference rather than by value. With this copied data table, we iterate over it looking for NAs in the `step` column. If we find an NA, we copy the corresponding `interval` median steps value from the previously created steps-per-interval table (`it`).

```{r}
dt_filled <- copy(dt)

## There has to be a more functional and more efficient way to do this,
##   but I'm at a loss for it at the moment... advice appreciated :)
for (i in seq_len(nrow(dt_filled))) {
    if (is.na(dt_filled[i, steps])) {
        dt_filled[i]$steps <- it[interval == dt_filled[i, interval]]$median_steps
    }
}
```

Confirm we don't have any more NAs in the new data table.

```{r}
dt_filled[is.na(steps)]
```

Compare the summary for our filled data table with that of the original one to see how the statistics changed.

```{r}
summary(dt_filled)
summary(dt)
```

### 4. Make a histogram of the total number of steps taken each day, and calculate and report the mean and median total number of steps taken per day. Do these values differ from the estimates from the first part of the assignment? What is the impact of imputing missing data on the estimates of the total daily number of steps?

We follow the pattern used above to create the total steps histogram, but using our new filled/imputed steps data table.

```{r}
(ts_filled <- dt_filled %>%
    group_by(date) %>%
    summarise(total_steps = sum(steps)))
```

Let's check the stats for the data, and compare it to the non-imputed total steps data from before.

```{r}
summary(ts_filled)
summary(ts)
```

Our imputed total steps data now looks like this:

```{r}
(p_ts_filled <- ggplot(ts_filled) +
    geom_histogram(aes(total_steps), binwidth = 500) +
    labs(title = "Histogram of Imputed Total Steps per Day",
         x = "Total steps per day"))
```

The only noticible difference is a new histogram column around 1000-1500 total steps.

Here are our imputed mean and median statistics:

```{r}
(ts_filled_mean <- mean(ts_filled$total_steps))
(ts_filled_median <- median(ts_filled$total_steps))
```

Let's compare our imputed mean and median with the previous non-imputed mean and median. The non-imputed mean and median are the vertical grey dashed and solid lines respectively, and our imputed mean and median are the vertical red dashed and solid lines respectively.

```{r}
(p_ts_filled_stats <- p_ts_filled + 
    geom_vline(aes(xintercept = ts_filled_median), color = "red", linetype = "solid") +
    geom_vline(aes(xintercept = ts_filled_mean), color = "red", linetype = "dashed") +
    geom_vline(aes(xintercept = ts_median), color = "grey50", linetype = "solid") +
    geom_vline(aes(xintercept = ts_mean), color = "grey50", linetype = "dashed"))
```

We can see that imputing with the median steps reduced our mean and median somewhat, and increased the gap between their values.

Just for comparison, here are the non-imputed histogram and imputed histogram side-by-side.

```{r, fig.width = 8}
## adapted from Hadley Wickham's "ggplot2" book, p. 154
library(grid, warn.conflicts = FALSE)
pushViewport(viewport(layout = grid.layout(1, 2)))
vplayout <- function(x, y)
  viewport(layout.pos.row = x, layout.pos.col = y)
print(p_ts_stats + ylim(0,8), vp = vplayout(1, 1))
print(p_ts_filled_stats, vp = vplayout(1, 2))
```

## Are there differences in activity patterns between weekdays and weekends?

### 1. Create a new factor variable in the dataset with two levels – “weekday” and “weekend” indicating whether a given date is a weekday or weekend day.

We copy our original data table, adding new columns identifying the day of the week (`DOW`) for the `date`, and then categorizing those `DOW`s as either weekday or weekend in a new column for `day_type`. We then factorize those columns, including ordering the `DOW` factors from Sunday to Saturday.

```{r}
(dt_days <- dt %>%
    mutate(DOW = weekdays(date)) %>%
    mutate(day_type = ifelse(DOW %in% c("Saturday", "Sunday"), 
                             "weekend", 
                             "weekday")))

dt_days$DOW <- factor(dt_days$DOW, levels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"), ordered = TRUE)
dt_days$day_type <- as.factor(dt_days$day_type)
```

Let's take a peek at the structure and summary of our new data table to make sure it column types look good, and to see counts of weekdays vs. weekends in the data.

```{r}
str(dt_days)
summary(dt_days)
```

### 2. Make a panel plot containing a time series plot (i.e. type = "l") of the 5-minute interval (x-axis) and the average number of steps taken, averaged across all weekday days or weekend days (y-axis). (See the README file in the GitHub repository to see an example of what this plot should look like using simulated data.)

Because we're using the original data with NAs, we'll follow our previous pattern of omitting the NA rows. To get the requested averages, we first group by `day_type` (weekday or weekend) and then by `interval` as we did for average interval steps before. We can then get the average steps per interval per type of day.

```{r}
(dit <- na.omit(dt_days) %>%
    group_by(day_type, interval) %>%
    summarise(avg_steps = mean(steps)))
```

For consistency, let's check out the summary info for our data.

```{r}
summary(dit)
```

We can now plot the interval averages per `day_type`. We use `ggplot2`'s facets to create the panel plot of weekday vs. weekend interval averages.

```{r}
ggplot(dit, aes(as.factor(interval), avg_steps)) +
    geom_line(aes(group = 1)) +
    scale_x_discrete(breaks = seq(0, 2400, 200)) +
    facet_grid(day_type ~ .) +
    labs(title = "Average Steps per Interval, Weekday vs. Weekend",
         x = "Intervals (time of day, 5 minute intervals)",
         y = "Average steps")
```
